{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Evaluation Code\n",
    "\n",
    "This notebook will be very __similar__ to the code I use to evaluate your results - it is provided for __your convenience__ so that you can use it to evaluate your preprocessing results at any time before your __final submission__.\n",
    "\n",
    "Please note that the results here will __NOT__ be the same as my evaluation results.\n",
    "\n",
    "Let's start with loading the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import required package for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import required packages for splitting data\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import required packages for evaluating models\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# import `logistic regression` model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, skew\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import xgboost\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you should load __your__ data. In this case, I am using a sample dataset (`GroupX.csv`) which contains 6 predictors (`X1 - X6`) and two target variables (`Y1, Y2`).\n",
    "\n",
    "Please make sure you change the data to your __OWN__ dataset when using this code.\n",
    "\n",
    "__NOTE__:\n",
    "1. Your dataset maybe very different from the sample dataset.\n",
    "2. Please follow this structure when submitting your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C3_imputed</th>\n",
       "      <th>C4</th>\n",
       "      <th>C7_imputed</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>real_word_per</th>\n",
       "      <th>non_real_word_per</th>\n",
       "      <th>short_sentences</th>\n",
       "      <th>short_sentences_per</th>\n",
       "      <th>long_sentences_per</th>\n",
       "      <th>...</th>\n",
       "      <th>outstanding_share_per_iqr_standardized_normalized</th>\n",
       "      <th>offering_share_per_iqr_standardized_normalized</th>\n",
       "      <th>C5_prime_iqr_standardized_normalized</th>\n",
       "      <th>C6_prime_iqr_standardized_normalized</th>\n",
       "      <th>C2</th>\n",
       "      <th>data_updated</th>\n",
       "      <th>C3_prime</th>\n",
       "      <th>industry_bin__Manufacturing</th>\n",
       "      <th>industry_bin__Other</th>\n",
       "      <th>industry_bin__Services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053924</td>\n",
       "      <td>0.169794</td>\n",
       "      <td>0.749962</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.372945</td>\n",
       "      <td>0.944727</td>\n",
       "      <td>0.055273</td>\n",
       "      <td>0.200306</td>\n",
       "      <td>0.224992</td>\n",
       "      <td>0.775008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139724</td>\n",
       "      <td>-0.127188</td>\n",
       "      <td>0.043286</td>\n",
       "      <td>0.920862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119884</td>\n",
       "      <td>0.168708</td>\n",
       "      <td>0.583748</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.394163</td>\n",
       "      <td>0.934175</td>\n",
       "      <td>0.065825</td>\n",
       "      <td>0.371560</td>\n",
       "      <td>0.219755</td>\n",
       "      <td>0.780245</td>\n",
       "      <td>...</td>\n",
       "      <td>2.201454</td>\n",
       "      <td>-2.265930</td>\n",
       "      <td>1.745398</td>\n",
       "      <td>-0.448096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038517</td>\n",
       "      <td>0.168790</td>\n",
       "      <td>0.717213</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.334588</td>\n",
       "      <td>0.945220</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>0.229360</td>\n",
       "      <td>0.770640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217267</td>\n",
       "      <td>0.226597</td>\n",
       "      <td>-0.308389</td>\n",
       "      <td>-2.056332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095811</td>\n",
       "      <td>0.168861</td>\n",
       "      <td>0.714502</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.291162</td>\n",
       "      <td>0.953234</td>\n",
       "      <td>0.046766</td>\n",
       "      <td>0.172783</td>\n",
       "      <td>0.346964</td>\n",
       "      <td>0.653036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273287</td>\n",
       "      <td>0.281867</td>\n",
       "      <td>-0.359440</td>\n",
       "      <td>-1.170267</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033702</td>\n",
       "      <td>0.169072</td>\n",
       "      <td>0.499347</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.323613</td>\n",
       "      <td>0.923515</td>\n",
       "      <td>0.076485</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.289146</td>\n",
       "      <td>0.710854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047082</td>\n",
       "      <td>-0.035090</td>\n",
       "      <td>-0.052653</td>\n",
       "      <td>0.537671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         C1  C3_imputed        C4  C7_imputed  words_per_sentence  \\\n",
       "0  0.053924    0.169794  0.749962    0.001671            0.372945   \n",
       "1  0.119884    0.168708  0.583748    0.000843            0.394163   \n",
       "2  0.038517    0.168790  0.717213    0.000238            0.334588   \n",
       "3  0.095811    0.168861  0.714502    0.000275            0.291162   \n",
       "4  0.033702    0.169072  0.499347    0.020605            0.323613   \n",
       "\n",
       "   real_word_per  non_real_word_per  short_sentences  short_sentences_per  \\\n",
       "0       0.944727           0.055273         0.200306             0.224992   \n",
       "1       0.934175           0.065825         0.371560             0.219755   \n",
       "2       0.945220           0.054780         0.053517             0.229360   \n",
       "3       0.953234           0.046766         0.172783             0.346964   \n",
       "4       0.923515           0.076485         0.302752             0.289146   \n",
       "\n",
       "   long_sentences_per  ...  outstanding_share_per_iqr_standardized_normalized  \\\n",
       "0            0.775008  ...                                           0.139724   \n",
       "1            0.780245  ...                                           2.201454   \n",
       "2            0.770640  ...                                          -0.217267   \n",
       "3            0.653036  ...                                          -0.273287   \n",
       "4            0.710854  ...                                           0.047082   \n",
       "\n",
       "   offering_share_per_iqr_standardized_normalized  \\\n",
       "0                                       -0.127188   \n",
       "1                                       -2.265930   \n",
       "2                                        0.226597   \n",
       "3                                        0.281867   \n",
       "4                                       -0.035090   \n",
       "\n",
       "   C5_prime_iqr_standardized_normalized  C6_prime_iqr_standardized_normalized  \\\n",
       "0                              0.043286                              0.920862   \n",
       "1                              1.745398                             -0.448096   \n",
       "2                             -0.308389                             -2.056332   \n",
       "3                             -0.359440                             -1.170267   \n",
       "4                             -0.052653                              0.537671   \n",
       "\n",
       "    C2  data_updated  C3_prime  industry_bin__Manufacturing  \\\n",
       "0  1.0             0         1                            1   \n",
       "1  0.0             0         0                            1   \n",
       "2  1.0             0         0                            1   \n",
       "3  1.0             0         0                            1   \n",
       "4  1.0             0         1                            0   \n",
       "\n",
       "   industry_bin__Other  industry_bin__Services  \n",
       "0                    0                       0  \n",
       "1                    0                       0  \n",
       "2                    0                       0  \n",
       "3                    0                       0  \n",
       "4                    1                       0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mat = pd.read_csv('feat_mat.csv', header=0).drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "tar = pd.read_csv('target_df.csv')\n",
    "feat_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to specify your targets and predictors. __NOTE__ we have two targets here (`Y1, Y2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = tar.Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very possible that you will use different sets of the predictors for `Y1` and `Y2`. Now let's define them.\n",
    "\n",
    "First, let's define predictors for `Y1` - which will be the first 5 features in `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use below code to select the first 5 features as predictors for `Y1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon investigation of the data, we know we have __six__ features (`X1 - X6`) predicting `Y2`. Use similar code (as below) to select them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features_rfe(xTrain, yTrain, num_feats):\n",
    "    '''\n",
    "    Takes training values and returns the most important features in order\n",
    "    '''\n",
    "    clf = LogisticRegression(random_state=123, max_iter = 1000)\n",
    "    selector = RFE(clf, num_feats, step=1)\n",
    "    selector.fit(xTrain, yTrain)\n",
    "    \n",
    "    return [x for _,x in sorted(zip(selector.ranking_, list(xTrain.columns)))]\n",
    "    \n",
    "    \n",
    "feat_rfe = get_important_features_rfe(feat_mat[[i for i in list(feat_mat.columns) \n",
    "                                     if i != 'I1'and 'iqr_standardized_normalized' in i]],\n",
    "                          y2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.6741; AUC 0.6122 \n",
      "Accuracy of classifier on test set: 0.659\n",
      "10-fold cross validation average accuracy of clf1: 0.620\n",
      "Confusion Matrix for Classfier:\n",
      "[[17 16]\n",
      " [29 70]]\n",
      "Classification Report for Classfier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.52      0.43        33\n",
      "           1       0.81      0.71      0.76        99\n",
      "\n",
      "    accuracy                           0.66       132\n",
      "   macro avg       0.59      0.61      0.59       132\n",
      "weighted avg       0.70      0.66      0.68       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictors_y2 = feat_mat[feat_rfe[:10]]\n",
    "\n",
    "# lists for f1-score and AUC\n",
    "f1_score_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "\n",
    "#loop to calculate f1 and auc scores and present averages after 10 runs\n",
    "for count in range (1,10):\n",
    "    #Model building\n",
    "    clf1 = LogisticRegression()\n",
    "\n",
    "    \n",
    "    # Splitting data into testing and training\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(predictors_y2, y2, test_size=0.2, random_state=123)\n",
    "    \n",
    "    # Begin oversampling\n",
    "    oversample = pd.concat([X2_train,y2_train],axis=1)\n",
    "    max_size = oversample['Y2'].value_counts().max()\n",
    "    lst = [oversample]\n",
    "    for class_index, group in oversample.groupby('Y2'):\n",
    "        lst.append(group.sample(max_size-len(group), replace=True))\n",
    "    X2_train = pd.concat(lst)\n",
    "    y2_train=pd.DataFrame.copy(X2_train['Y2'])\n",
    "    del X2_train['Y2']\n",
    "    \n",
    "    # fitting model on oversampled data\n",
    "    clf1.fit(X2_train, y2_train)\n",
    "    \n",
    "    y2_pred = clf1.predict(X2_test)\n",
    "    \n",
    "    \n",
    "    #10-fold cross validation\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=123)\n",
    "    scoring = 'accuracy'\n",
    "    results = model_selection.cross_val_score(clf1, X2_train, y2_train, cv=kfold, scoring=scoring)\n",
    "    \n",
    "    #calculate f1-score and AUC\n",
    "    \n",
    "    clf1_roc_auc = roc_auc_score(y2_test, y2_pred)\n",
    "    \n",
    "    \n",
    "    #calculate average f1-score and AUC\n",
    "    f1_score_lst.append(precision_recall_fscore_support(y2_test, y2_pred, average='weighted')[2])\n",
    "    auc_lst.append(clf1_roc_auc)\n",
    "    \n",
    "    \n",
    "print('F1 {:.4f}; AUC {:.4f} '.format(np.mean(f1_score_lst),np.mean(auc_lst)))\n",
    "\n",
    "confusion_matrix_y2 = confusion_matrix(y2_test, y2_pred)\n",
    "\n",
    "\n",
    "print('Accuracy of classifier on test set: {:.3f}'.format(clf1.score(X2_test, y2_test)))\n",
    "\n",
    "print(\"10-fold cross validation average accuracy of clf1: %.3f\" % (results.mean()))\n",
    "\n",
    "print('Confusion Matrix for Classfier:')\n",
    "print(confusion_matrix_y2)\n",
    "\n",
    "print('Classification Report for Classfier:')\n",
    "print(classification_report(y2_test, y2_pred))\n",
    "\n",
    "imp = pd.DataFrame(zip(predictors_y2.columns, clf1.coef_[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>outstanding_share_per_iqr_standardized_normalized</td>\n",
       "      <td>-0.113440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>real_word_per_iqr_standardized_normalized</td>\n",
       "      <td>-0.081528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non_real_word_per_iqr_standardized_normalized</td>\n",
       "      <td>-0.073219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>short_sentences_iqr_standardized_normalized</td>\n",
       "      <td>-0.063225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos_words_percent_iqr_standardized_normalized</td>\n",
       "      <td>0.061861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uncertain_words_percent_iqr_standardized_norma...</td>\n",
       "      <td>0.146997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C4_iqr_standardized_normalized</td>\n",
       "      <td>0.206251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offering_share_per_iqr_standardized_normalized</td>\n",
       "      <td>0.316794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C6_prime_iqr_standardized_normalized</td>\n",
       "      <td>0.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C5_prime_iqr_standardized_normalized</td>\n",
       "      <td>0.630189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               field       imp\n",
       "5  outstanding_share_per_iqr_standardized_normalized -0.113440\n",
       "7          real_word_per_iqr_standardized_normalized -0.081528\n",
       "3      non_real_word_per_iqr_standardized_normalized -0.073219\n",
       "8        short_sentences_iqr_standardized_normalized -0.063225\n",
       "6      pos_words_percent_iqr_standardized_normalized  0.061861\n",
       "9  uncertain_words_percent_iqr_standardized_norma...  0.146997\n",
       "0                     C4_iqr_standardized_normalized  0.206251\n",
       "4     offering_share_per_iqr_standardized_normalized  0.316794\n",
       "2               C6_prime_iqr_standardized_normalized  0.564100\n",
       "1               C5_prime_iqr_standardized_normalized  0.630189"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.columns = ['field', 'imp']\n",
    "imp.sort_values(by = 'imp')\n",
    "imp.sort_values('imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C4_iqr_standardized_normalized</th>\n",
       "      <th>C5_prime_iqr_standardized_normalized</th>\n",
       "      <th>C6_prime_iqr_standardized_normalized</th>\n",
       "      <th>non_real_word_per_iqr_standardized_normalized</th>\n",
       "      <th>offering_share_per_iqr_standardized_normalized</th>\n",
       "      <th>outstanding_share_per_iqr_standardized_normalized</th>\n",
       "      <th>pos_words_percent_iqr_standardized_normalized</th>\n",
       "      <th>real_word_per_iqr_standardized_normalized</th>\n",
       "      <th>short_sentences_iqr_standardized_normalized</th>\n",
       "      <th>uncertain_words_percent_iqr_standardized_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-1.986528</td>\n",
       "      <td>-0.702535</td>\n",
       "      <td>0.227854</td>\n",
       "      <td>0.105486</td>\n",
       "      <td>0.682907</td>\n",
       "      <td>-0.681289</td>\n",
       "      <td>-1.185910</td>\n",
       "      <td>-0.091443</td>\n",
       "      <td>-0.881927</td>\n",
       "      <td>-0.885924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.052305</td>\n",
       "      <td>1.476454</td>\n",
       "      <td>0.850828</td>\n",
       "      <td>1.744737</td>\n",
       "      <td>-1.205474</td>\n",
       "      <td>1.204023</td>\n",
       "      <td>-0.915965</td>\n",
       "      <td>-1.767273</td>\n",
       "      <td>-0.945286</td>\n",
       "      <td>2.061087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-1.102548</td>\n",
       "      <td>0.191677</td>\n",
       "      <td>0.554070</td>\n",
       "      <td>0.925739</td>\n",
       "      <td>-0.263660</td>\n",
       "      <td>0.276583</td>\n",
       "      <td>-1.870109</td>\n",
       "      <td>-0.934757</td>\n",
       "      <td>-0.205251</td>\n",
       "      <td>-2.056006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.082544</td>\n",
       "      <td>0.747594</td>\n",
       "      <td>-0.280780</td>\n",
       "      <td>1.733440</td>\n",
       "      <td>-0.718293</td>\n",
       "      <td>0.728345</td>\n",
       "      <td>0.424286</td>\n",
       "      <td>-1.756046</td>\n",
       "      <td>-0.494828</td>\n",
       "      <td>-0.567506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.705530</td>\n",
       "      <td>0.464080</td>\n",
       "      <td>0.705885</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.496854</td>\n",
       "      <td>0.509157</td>\n",
       "      <td>-1.783850</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>-0.554385</td>\n",
       "      <td>2.061087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-1.027777</td>\n",
       "      <td>1.745398</td>\n",
       "      <td>0.603822</td>\n",
       "      <td>1.177085</td>\n",
       "      <td>-1.666626</td>\n",
       "      <td>1.644745</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>-1.193086</td>\n",
       "      <td>-0.319479</td>\n",
       "      <td>0.681187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-1.419823</td>\n",
       "      <td>-1.102998</td>\n",
       "      <td>0.938617</td>\n",
       "      <td>-1.275447</td>\n",
       "      <td>1.227557</td>\n",
       "      <td>-1.237775</td>\n",
       "      <td>0.386357</td>\n",
       "      <td>1.270932</td>\n",
       "      <td>0.866624</td>\n",
       "      <td>-0.553741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.961533</td>\n",
       "      <td>-0.839893</td>\n",
       "      <td>-0.837195</td>\n",
       "      <td>-0.835533</td>\n",
       "      <td>0.859497</td>\n",
       "      <td>-0.861561</td>\n",
       "      <td>0.249806</td>\n",
       "      <td>0.848932</td>\n",
       "      <td>-0.224144</td>\n",
       "      <td>0.306613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-1.378213</td>\n",
       "      <td>1.745398</td>\n",
       "      <td>-1.472387</td>\n",
       "      <td>-0.921725</td>\n",
       "      <td>-1.564697</td>\n",
       "      <td>1.548202</td>\n",
       "      <td>-0.158397</td>\n",
       "      <td>0.932630</td>\n",
       "      <td>1.161151</td>\n",
       "      <td>-1.149096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.961533</td>\n",
       "      <td>-0.839893</td>\n",
       "      <td>-0.837195</td>\n",
       "      <td>-0.835533</td>\n",
       "      <td>0.859497</td>\n",
       "      <td>-0.861561</td>\n",
       "      <td>0.249806</td>\n",
       "      <td>0.848932</td>\n",
       "      <td>-0.224144</td>\n",
       "      <td>0.306613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     C4_iqr_standardized_normalized  C5_prime_iqr_standardized_normalized  \\\n",
       "303                       -1.986528                             -0.702535   \n",
       "75                         0.052305                              1.476454   \n",
       "267                       -1.102548                              0.191677   \n",
       "491                        0.082544                              0.747594   \n",
       "101                       -0.705530                              0.464080   \n",
       "..                              ...                                   ...   \n",
       "628                       -1.027777                              1.745398   \n",
       "599                       -1.419823                             -1.102998   \n",
       "247                        0.961533                             -0.839893   \n",
       "251                       -1.378213                              1.745398   \n",
       "247                        0.961533                             -0.839893   \n",
       "\n",
       "     C6_prime_iqr_standardized_normalized  \\\n",
       "303                              0.227854   \n",
       "75                               0.850828   \n",
       "267                              0.554070   \n",
       "491                             -0.280780   \n",
       "101                              0.705885   \n",
       "..                                    ...   \n",
       "628                              0.603822   \n",
       "599                              0.938617   \n",
       "247                             -0.837195   \n",
       "251                             -1.472387   \n",
       "247                             -0.837195   \n",
       "\n",
       "     non_real_word_per_iqr_standardized_normalized  \\\n",
       "303                                       0.105486   \n",
       "75                                        1.744737   \n",
       "267                                       0.925739   \n",
       "491                                       1.733440   \n",
       "101                                      -0.002472   \n",
       "..                                             ...   \n",
       "628                                       1.177085   \n",
       "599                                      -1.275447   \n",
       "247                                      -0.835533   \n",
       "251                                      -0.921725   \n",
       "247                                      -0.835533   \n",
       "\n",
       "     offering_share_per_iqr_standardized_normalized  \\\n",
       "303                                        0.682907   \n",
       "75                                        -1.205474   \n",
       "267                                       -0.263660   \n",
       "491                                       -0.718293   \n",
       "101                                       -0.496854   \n",
       "..                                              ...   \n",
       "628                                       -1.666626   \n",
       "599                                        1.227557   \n",
       "247                                        0.859497   \n",
       "251                                       -1.564697   \n",
       "247                                        0.859497   \n",
       "\n",
       "     outstanding_share_per_iqr_standardized_normalized  \\\n",
       "303                                          -0.681289   \n",
       "75                                            1.204023   \n",
       "267                                           0.276583   \n",
       "491                                           0.728345   \n",
       "101                                           0.509157   \n",
       "..                                                 ...   \n",
       "628                                           1.644745   \n",
       "599                                          -1.237775   \n",
       "247                                          -0.861561   \n",
       "251                                           1.548202   \n",
       "247                                          -0.861561   \n",
       "\n",
       "     pos_words_percent_iqr_standardized_normalized  \\\n",
       "303                                      -1.185910   \n",
       "75                                       -0.915965   \n",
       "267                                      -1.870109   \n",
       "491                                       0.424286   \n",
       "101                                      -1.783850   \n",
       "..                                             ...   \n",
       "628                                       0.011439   \n",
       "599                                       0.386357   \n",
       "247                                       0.249806   \n",
       "251                                      -0.158397   \n",
       "247                                       0.249806   \n",
       "\n",
       "     real_word_per_iqr_standardized_normalized  \\\n",
       "303                                  -0.091443   \n",
       "75                                   -1.767273   \n",
       "267                                  -0.934757   \n",
       "491                                  -1.756046   \n",
       "101                                   0.018488   \n",
       "..                                         ...   \n",
       "628                                  -1.193086   \n",
       "599                                   1.270932   \n",
       "247                                   0.848932   \n",
       "251                                   0.932630   \n",
       "247                                   0.848932   \n",
       "\n",
       "     short_sentences_iqr_standardized_normalized  \\\n",
       "303                                    -0.881927   \n",
       "75                                     -0.945286   \n",
       "267                                    -0.205251   \n",
       "491                                    -0.494828   \n",
       "101                                    -0.554385   \n",
       "..                                           ...   \n",
       "628                                    -0.319479   \n",
       "599                                     0.866624   \n",
       "247                                    -0.224144   \n",
       "251                                     1.161151   \n",
       "247                                    -0.224144   \n",
       "\n",
       "     uncertain_words_percent_iqr_standardized_normalized  \n",
       "303                                          -0.885924    \n",
       "75                                            2.061087    \n",
       "267                                          -2.056006    \n",
       "491                                          -0.567506    \n",
       "101                                           2.061087    \n",
       "..                                                 ...    \n",
       "628                                           0.681187    \n",
       "599                                          -0.553741    \n",
       "247                                           0.306613    \n",
       "251                                          -1.149096    \n",
       "247                                           0.306613    \n",
       "\n",
       "[724 rows x 10 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "pca_transformed = pca.fit_transform(feat_mat[[i for i in feat_rfe[:10]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.226255</td>\n",
       "      <td>-0.132458</td>\n",
       "      <td>0.228051</td>\n",
       "      <td>-0.835661</td>\n",
       "      <td>1.461639</td>\n",
       "      <td>-1.022270</td>\n",
       "      <td>1.719716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.927918</td>\n",
       "      <td>-2.241339</td>\n",
       "      <td>2.962121</td>\n",
       "      <td>-0.205626</td>\n",
       "      <td>-0.224036</td>\n",
       "      <td>-0.239944</td>\n",
       "      <td>-0.491821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071858</td>\n",
       "      <td>-1.697794</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>1.412772</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>2.493536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.703054</td>\n",
       "      <td>-2.672157</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>-0.608705</td>\n",
       "      <td>-0.553440</td>\n",
       "      <td>0.428641</td>\n",
       "      <td>0.156327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.058241</td>\n",
       "      <td>0.388617</td>\n",
       "      <td>2.608713</td>\n",
       "      <td>-0.062291</td>\n",
       "      <td>1.018400</td>\n",
       "      <td>-0.579937</td>\n",
       "      <td>-0.030870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-1.761582</td>\n",
       "      <td>0.353911</td>\n",
       "      <td>-0.578827</td>\n",
       "      <td>1.489498</td>\n",
       "      <td>0.103392</td>\n",
       "      <td>1.290776</td>\n",
       "      <td>-0.390665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.426711</td>\n",
       "      <td>-0.171188</td>\n",
       "      <td>1.026408</td>\n",
       "      <td>-0.009798</td>\n",
       "      <td>0.479818</td>\n",
       "      <td>-0.278247</td>\n",
       "      <td>-0.531803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>-0.784074</td>\n",
       "      <td>-0.612558</td>\n",
       "      <td>-2.258011</td>\n",
       "      <td>-0.664215</td>\n",
       "      <td>0.514634</td>\n",
       "      <td>0.301464</td>\n",
       "      <td>0.632781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>-0.832929</td>\n",
       "      <td>0.543464</td>\n",
       "      <td>-1.837011</td>\n",
       "      <td>0.379254</td>\n",
       "      <td>-1.326105</td>\n",
       "      <td>0.924952</td>\n",
       "      <td>-0.345398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>1.568470</td>\n",
       "      <td>-1.190729</td>\n",
       "      <td>1.706527</td>\n",
       "      <td>-1.836700</td>\n",
       "      <td>0.305575</td>\n",
       "      <td>-0.864168</td>\n",
       "      <td>0.450371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>685 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6\n",
       "0    1.226255 -0.132458  0.228051 -0.835661  1.461639 -1.022270  1.719716\n",
       "1   -1.927918 -2.241339  2.962121 -0.205626 -0.224036 -0.239944 -0.491821\n",
       "2   -0.071858 -1.697794 -0.012033  0.311111  1.412772  0.011048  2.493536\n",
       "3   -0.703054 -2.672157  0.077995 -0.608705 -0.553440  0.428641  0.156327\n",
       "4   -1.058241  0.388617  2.608713 -0.062291  1.018400 -0.579937 -0.030870\n",
       "..        ...       ...       ...       ...       ...       ...       ...\n",
       "52  -1.761582  0.353911 -0.578827  1.489498  0.103392  1.290776 -0.390665\n",
       "97  -0.426711 -0.171188  1.026408 -0.009798  0.479818 -0.278247 -0.531803\n",
       "343 -0.784074 -0.612558 -2.258011 -0.664215  0.514634  0.301464  0.632781\n",
       "237 -0.832929  0.543464 -1.837011  0.379254 -1.326105  0.924952 -0.345398\n",
       "428  1.568470 -1.190729  1.706527 -1.836700  0.305575 -0.864168  0.450371\n",
       "\n",
       "[685 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-bdd09f4e6ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# fitting model on oversampled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "predictors_y2 = pca_transformed\n",
    "\n",
    "# lists for f1-score and AUC\n",
    "f1_score_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "\n",
    "#loop to calculate f1 and auc scores and present averages after 10 runs\n",
    "for count in range (1,10):\n",
    "    #Model building\n",
    "    clf1 = LogisticRegression()\n",
    "\n",
    "    \n",
    "    # Splitting data into testing and training\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(predictors_y2, y2, test_size=0.2, random_state=123)\n",
    "    X2_train = pd.DataFrame(X2_train)\n",
    "    X2_train['Y2'] = y2_train\n",
    "    # Begin oversampling\n",
    "    oversample = X2_train\n",
    "    max_size = oversample['Y2'].value_counts().max()\n",
    "    lst = [oversample]\n",
    "    for class_index, group in oversample.groupby('Y2'):\n",
    "        lst.append(group.sample(max_size-len(group), replace=True))\n",
    "    X2_train = pd.concat(lst)\n",
    "    y2_train=pd.DataFrame.copy(X2_train['Y2'])\n",
    "    del X2_train['Y2']\n",
    "    \n",
    "    # fitting model on oversampled data\n",
    "    clf1.fit(pd.DataFrame(X2_train), y2_train)\n",
    "    \n",
    "    y2_pred = clf1.predict(X2_test)\n",
    "    \n",
    "    \n",
    "    #10-fold cross validation\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=123)\n",
    "    scoring = 'accuracy'\n",
    "    results = model_selection.cross_val_score(clf1, X2_train, y2_train, cv=kfold, scoring=scoring)\n",
    "    \n",
    "    #calculate f1-score and AUC\n",
    "    \n",
    "    clf1_roc_auc = roc_auc_score(y2_test, y2_pred)\n",
    "    \n",
    "    \n",
    "    #calculate average f1-score and AUC\n",
    "    f1_score_lst.append(precision_recall_fscore_support(y2_test, y2_pred, average='weighted')[2])\n",
    "    auc_lst.append(clf1_roc_auc)\n",
    "    \n",
    "    \n",
    "print('F1 {:.4f}; AUC {:.4f} '.format(np.mean(f1_score_lst),np.mean(auc_lst)))\n",
    "\n",
    "confusion_matrix_y2 = confusion_matrix(y2_test, y2_pred)\n",
    "\n",
    "\n",
    "print('Accuracy of classifier on test set: {:.3f}'.format(clf1.score(X2_test, y2_test)))\n",
    "\n",
    "print(\"10-fold cross validation average accuracy of clf1: %.3f\" % (results.mean()))\n",
    "\n",
    "print('Confusion Matrix for Classfier:')\n",
    "print(confusion_matrix_y2)\n",
    "\n",
    "print('Classification Report for Classfier:')\n",
    "print(classification_report(y2_test, y2_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train['Y2'] = y2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1b36479d329f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# fitting model on oversampled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0my2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "\n",
    "\n",
    "# Splitting data into testing and training\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(predictors_y2, y2, test_size=0.2, random_state=123)\n",
    "X2_train = pd.DataFrame(X2_train)\n",
    "X2_train['Y2'] = y2_train\n",
    "# Begin oversampling\n",
    "oversample = X2_train\n",
    "max_size = oversample['Y2'].value_counts().max()\n",
    "lst = [oversample]\n",
    "for class_index, group in oversample.groupby('Y2'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "X2_train = pd.concat(lst)\n",
    "y2_train=pd.DataFrame.copy(X2_train['Y2'])\n",
    "del X2_train['Y2']\n",
    "\n",
    "# fitting model on oversampled data\n",
    "clf1.fit(pd.DataFrame(X2_train), y2_train)\n",
    "\n",
    "y2_pred = clf1.predict(X2_test)\n",
    "\n",
    "\n",
    "#10-fold cross validation\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=123)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(clf1, X2_train, y2_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "#calculate f1-score and AUC\n",
    "\n",
    "clf1_roc_auc = roc_auc_score(y2_test, y2_pred)\n",
    "\n",
    "\n",
    "#calculate average f1-score and AUC\n",
    "f1_score_lst.append(precision_recall_fscore_support(y2_test, y2_pred, average='weighted')[2])\n",
    "auc_lst.append(clf1_roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
